\section{Numerical Integration Techniques}

\subsection{Linear Dynamics}

The nonlinear dynamics formulated above can be placed into standard
nonlinear affine form as shown below after much simplification of
terms
\begin{equation}
  \dot{\vec{x}} = \vec{f}(\vec{x}) + \vec{g}(\vec{x})\vec{u}
\end{equation}
where $\vec{u}$ is the control input which could be the forces and
moments from reaction wheels or thrusters. The equation above can be
linearized to give the equation below. 
\begin{equation}
  \Delta \dot{\vec{x}} = {\bf A}\Delta {\vec{x}} + {\bf B}\Delta \vec{u}
\end{equation}
where $\Delta \vec{x} = \vec{x} - \vec{x}_e$ and $\vec{x}_e$ is an
equilibrium point. In this formulation ${\bf A} = \partial \vec{f}/\partial \vec{x}$. and 
${\bf B} = \partial \vec{g}/\partial \vec{x}$

\subsection{Euler's Method}

The equations of motion above can be integrated using Euler's method
which is a crude first order method to approximate the time series
solution \cite{Chapra_MEANALYSIS}. Note that this method is prone to a 
significant amount of instability unless the timestep is very small. 
\begin{equation}
  \begin{matrix}
    \vec{x}_{k+1} = \vec{x}_k + \dot{\vec{x}}(t_k,\vec{x}_k) \Delta t \\
    \dot{\vec{x}}(t_k,\vec{x}_k) = \vec{f}(\vec{x}_k) + \vec{g}(\vec{x}_k)\vec{u}_k
  \end{matrix}
\end{equation}

\subsection{Runge-Kutta-4}

The RK4 algorithm is the standard in numerical integration and is
given in the equation below \cite{Chapra_MEANALYSIS}. The derivative
of the quaternions is the same in RK4 as it is in Euler's method. This
method is superior to RK4 in that it will converge faster as a
function of timestep.
\begin{equation}
  \begin{matrix}
    \vec{k}_1 = \dot{\vec{x}}(t_k,\vec{x}_k)\\
    \vec{k}_2 = \dot{\vec{x}}(t_k+\Delta t/2,\vec{x}_k+\vec{k}_1\Delta t/2)\\
    \vec{k}_3 = \dot{\vec{x}}(t_k+\Delta t/2,\vec{x}_k+\vec{k}_2\Delta t/2)\\
    \vec{k}_4 = \dot{\vec{x}}(t_k+\Delta t,\vec{x}_k+\vec{k}_3\Delta t)\\
    \vec{k} = \frac{1}{6}(\vec{k}_1 + 2\vec{k}_2 + 2\vec{k}_3 + \vec{k}_4)\\
    \vec{x}_{k+1} = \vec{x}_k + \vec{k} \Delta t \\
  \end{matrix}
\end{equation}

\subsection{Discrete Dynamics}

It is often useful for modern computers to write the equations of
motion in discrete form....

\section{State Estimation}

\subsection{Sensor Measurement} \label{s:measurements}

During the standard estimation procedure, it is assumed that
measurements are made that relate to the state or the state is
directly measured. If the state is directly measured like star trackers no special formulation need to
made. However, other sensors such as Sun sensors, magnetometers and
horizon sensors measure a vector in 3-D space. In general a
measurement $\bar{y}_k$ can be expressed by the nonlinear equation
shown below where $\vec{x}$ is the state vector. 
\begin{equation}
  \bar{y}_k = \vec{h}(\vec{x}_k) + \vec{\nu}_k
\end{equation}
The vector $\vec{\nu}_k$ is noise associated with the sensor
\cite{Munoz,cassidis}. If the system is linearized about some
equilibrium point the measurement equation can be written as
\begin{equation}
  \bar{y}_k = {\bf h}_k\vec{x}_k + \vec{\nu}_k
\end{equation}
where ${\bf h}_k = \partial \vec{h}/\partial \vec{x}$. 
It's easy to see here that in the case of the star tracker the matrix
${\bf h}_k$ is just the identity matrix. The noise vector
$\vec{\nu}_k$ is assumed to be gaussian white noise while the
covariance $cov()$ is given by the equation below using the
expectation operator ${\bf E}()$.
\begin{equation}
  cov(\vec{\nu}_k) = {\bf E}(\vec{\nu}_k\vec{\nu}_k^T) = {\bf R}_k
\end{equation}
If a measurement is made by a
Sun sensor or similar where a vector in 3-D space can be compared to a
known inertial reference vector the measurement update can be given as 
\begin{equation}
  {\bar{r}^B}_k = {\bf T}_{BI}(\vec{q}_k){\vec{r}^I}_k + \vec{\nu}_k
\end{equation}
where ${\bar{r}^B}_k$ is a measurement in the body reference frame at
time $t_k$. The angular velocity measurement in particular can be denoted as
$\bar{\omega}_k$. Measurements are typically polluted with bias and
white noise. For example, the angular velocity measurement can be
given as
\begin{equation}
  \bar{\omega} = \vec{\omega} + \vec{b} + \vec{\eta}_g
\end{equation}
where $\vec{b}$ is a bias that has dynamics given by
$\dot{\vec{b}}=\vec{\eta}_b$. The vectors $\vec{\eta}_g$ and
$\vec{\eta}_b$ are standard Gaussian white noise vectors. Typically
white noise can be filtered out using lowpass filters, complimentary
filters or even Kalman Filters while bias can just be
substracted. Thus, the estimate for the angular velocity can be
written as
\begin{equation}
  \tilde{\omega} = \bar{\omega}-\tilde{b}
\end{equation}
where $\tilde{b}$ is the estimate of the bias.

\subsection{Linear Least Squares}

In order to understand the nature of a Kalman filter, the linear least
squares solution is shown below. Assume for the moment that $M$
independent measurements are made such that $\bar{Y} = [\bar{y}_1,...,\bar{y}_M]^T$.
\begin{equation}
  \bar{Y} = {\bf H}\vec{x} + \vec{V}
\end{equation}
In this case ${\bf H} = [{\bf h}_1,...,{\bf h}_M]^T$ and $\vec{V} = [\nu_1,...,\nu_M]^T$. 
The vector $\vec{V}$ is a vector of error values between your
measurements and the actual truth signals $Y = {\bf H}\vec{x}$. Absent
of all measurement and model noise there 
would be a unique solution to this problem to solve for the vector
$\vec{x}$. The matrices $\bar{Y}$ and {\bf H} are
known and are the measurements and the output equation relating the
measurements to the state values in $\vec{x}$ respectively. Because of
measurement and model noise, a unique solution is not possible. That
is, the problem is overconstrained since typically the number of measurements is larger
than the number of unknowns. Take the linear example as shown in the
figure below.
\begin{figure}[H]
  \begin{center}
  \includegraphics[height=80mm, width=100mm]{Figures/Linear_Regression.png}
  \end{center}
  \caption{Linear Regression Example}\label{f:linear_regression}
\end{figure}
In this case the ordinate axis is the output $Y$ and the abscissa is
the independent variable that characterizes the matrix ${\bf H}$. The
black dots then are the measurements $\bar{Y}$ while the trend line is
the estimate $\tilde{Y} = {\bf H}\tilde{x}$. In this case the residuals
$\hat{Y} = \tilde{Y}-\bar{Y}$ is the distance between the trend line
in red and the black dots (the measurements). For this linear example,
the unknowns would be the slope and intercept. It is clear here that
there exists no linear solution $\vec{x}$ that goes through all black
data points. Thus, the equation below can be constructed.
\begin{equation}
  \bar{Y} = {\bf H}\tilde{x} + \hat{Y}
\end{equation}
This implies that the trendline $\tilde{Y}$ would go through all data
points if $\hat{Y}$ were zero. Thus the solution to this problem was
originally found by Gauss \cite{stigler1981} and involved minimizing
the residuals between $\bar{Y}$ and $\tilde{Y}$ (the estimated Y
values). To do this, a cost function is generated such that
\begin{equation}
  J = \frac{1}{2}\hat{Y}^T \hat{Y}
\end{equation}
Substituting in the equation $\hat{Y} = \bar{Y} - {\bf H}\tilde{x}$
and minimizing the cost function $\partial J/\partial \tilde{x} = 0$
results in the solution below.
\begin{equation}
  \tilde{x} = ({\bf H}^T{\bf H})^{-1}{\bf H}^T\bar{Y}
\end{equation}
Note that the equation above only works if the number of measurements $M$
is greater than or equal to the number of unknowns $N$. If not, the
solution will always be rank deficient and no solution will be
found. This is called an under constrained problem. In this there are
an infinite number of solutions that satisfy $\bar{Y} = {\bf
  H}\vec{x}$ even in the presence of modeling errors. In order
to get around this issue Lagrange's method of 
optimization is used \cite{lagrange}. For problems like this the
residuals between the estimate $\tilde{Y}$ and the measured signals
$\bar{Y}$ can be easily made to be zero. Thus minimizing the residuals
is trivial since the solution will still be an infinite number of
solutions. Therefore a constraint can be placed where
$\bar{Y}=\tilde{Y}={\bf H}\tilde{x}$. In order to find a unique
solution then the requirement is placed to minimize the estimate
$\tilde{x}$. In this case, the cost function
to be minimized is given by Lagrange's extension to optimization as
shown below
\begin{equation}
L = \frac{1}{2}\tilde{x}^T\tilde{x} + \lambda^T(\bar{Y}-{\bf H}\tilde{x})
\end{equation}
The cost function above utilizes the method of Lagrange multipliers in
order to satisfy the constraint that the solution must pass through
all measurements again only if the number of measurements $M$ is less than
the number of unknowns $N$. In the equation above the vector
$\tilde{x}$ must be solved and so must the Lagrange multipliers
$\lambda$. The solution to the problem above requires
$\partial L / \partial \tilde{x} = 0$ and $\partial L / \partial \lambda = 0$. Carrying out the partial derivatives and solving for the estimate
yields the following equations. 
\begin{equation}
  \tilde{x} = {\bf H}^T({\bf H}{\bf H}^T)^{-1}\bar{Y}
\end{equation}
Note, it is standard practice in state estimation to have at least as
many measurements as unknowns. In this case $M=N$ and Gauss' solution
is sufficient. 

\subsection{Weighted Least Squares}

The weighted least squares solution is found by setting the cost
function equal to $J = \frac{1}{2}\hat{Y}^T{\bf W}\hat{Y}$
where {\bf W} is a positive definite and symmetric weighting matrix. The solution then is
shown below.
\begin{equation}\label{e:weights}
  \tilde{x} = ({\bf H}^T{\bf W}{\bf H})^{-1}{\bf H}^T{\bf W}\bar{Y}
\end{equation}
In the standard Kalman Filter approach, the weighting matrix is given
by the inverse covariance of the error ${\bf r} = {\bf
  E}[\vec{v}\vec{v}^T]$. Placing this into a matrix yield ${\bf W} =
{\bf R}^{-1}$ where ${\bf R} = diag([{\bf r}_1,...{\bf r}_M])$.  
The weighted least squares solution then reduces to
\begin{equation}
  \tilde{x} = ({\bf H}^T{\bf R}^{-1}{\bf H})^{-1}{\bf H}^T{\bf R}^{-1}\bar{Y}
\end{equation}

\subsection{A Priori Knowledge of the State Vector}

If a priori knowledge is obtained via other means or in the case of
the standard Kalman Filter from integration of the state, it is
possible to obtain an updated estimate of the state based on the
previous state estimate and the new sensor measurements. First, the a
priori estimate $\tilde{x}^{-}$ is written as 
\begin{equation}
  \tilde{x}^{-} = \vec{x} +  \vec{w}
\end{equation}
where $\vec{w}$ is model noise associated with the error in the
state estimate. The covariance of this noise is also denoted as a
matrix and defined below.
\begin{equation}
  cov(\vec{w}) = {\bf E}(\vec{w}\vec{w}^T) = {\bf q}
\end{equation}
In this case it is desired for the updated measurement to be some
linear combination of the a priori equation and the measurements such
that
\begin{equation}
  \tilde{x} = {\bf \Lambda}\bar{Y} + {\bf \Gamma}\tilde{x}^{-}
\end{equation}
The matrices ${\bf \Lambda}$ and ${\bf \Gamma}$ have an added constraint
which can be shown by assuming the a priori measurement is perfect
$\tilde{x}^- = \vec{x}$ and the measurements $\bar{Y} = Y = {\bf
  H}\vec{x}$. In this case, we must have the updated estimate equal
the truth signal. $\tilde{x} = \vec{x}$. Rearranging the equation above yields
\begin{equation}
  \vec{x} = {\bf \Lambda}{\bf H}\vec{x} + {\bf \Gamma}\vec{x}
\end{equation}
which means that $({\bf \Lambda}{\bf H} + {\bf \Gamma}) = {\bf I}$
Again using the method of lagrange multipliers the cost function to be
minimized is given as
\begin{equation}
  L = {\bf E}[\frac{1}{2}\hat{x}^T\hat{x} + \lambda^T({\bf I} - {\bf
    \Lambda}{\bf H} - {\bf \Gamma})]
\end{equation}
where $\hat{x} = \tilde{x} - \vec{x}$ and again {\bf E} is the
expectation operator. Remembering that {\bf q} is
the covariance of the model noise and {\bf r} is the covariance of the
measurement noise, the solution to the minimization problem is given
by the equation below. 
\begin{equation}
  \tilde{x} = ({\bf H}^T{\bf R}^{-1}{\bf H}+{\bf q}^{-1})^{-1}({\bf
    H}^T{\bf R}^{-1}\bar{Y}+{\bf q}^{-1}\tilde{x}^{-})
\end{equation}
Note that this solution assumes that ${\bf E}(\vec{w}\vec{v}^T) = 0$. Measurement and model noise are uncorrelated.

\subsection{Complimentary Filter}

Looking at the equation for the A priori knowledge it is possible to
formulate the complimentary filter. First, the measurements are
assumed to be identical to the state vector such that ${\bf h}_k = {\bf
  I}$. From here a few extremes are shown below. First, assume that the measurement error is very low such
that the $cov(\vec{\nu})<<1$ while the model noise $\vec{w}$ is very
large approaching infinity. In this case, ${\bf q}^{-1} =0$. Substituting this into the weighted apriori equation yields
\begin{equation}
  \tilde{x} = average(\bar{Y})
\end{equation}
which essentially states that the estimate completely believes the
sensor measurement. If instead we assume that the model noise is
perfect such that $cov(\vec{w})<<1$ and the sensor noise is
approaching infinity, then ${\bf R}^{-1}=0$. This yields the following equation.
\begin{equation}
  \tilde{x} = \tilde{x}^{-}
\end{equation}
Thus it can be seen that there is a sliding bar between believing the
apriori estimate or the sensor measurement. As such it is possible to
develop a much simpler filter. First a constraint is placed on {\bf q}
and {\bf R} such that
\begin{equation}
  ({\bf H}^T{\bf R}^{-1}{\bf H}+{\bf q}^{-1}) = {\bf I}
\end{equation}
This causes the update law to reduce to the following
\begin{equation}
  \tilde{x} = {\bf H}^T{\bf R}^{-1}\bar{Y}+{\bf q}^{-1}\tilde{x}^{-}
\end{equation}
If only one measurement is investigated the equation collapses to the
following.
\begin{equation}
  \tilde{x} = {\bf r}^{-1}\bar{y}+{\bf q}^{-1}\tilde{x}^{-}
\end{equation}
The constraint also collapses to
\begin{equation}
  {\bf r}^{-1}+{\bf q}^{-1} = {\bf 1}
\end{equation}
If ${\bf q}^{-1} = {\bf s}$ and ${\bf r}^{-1}={\bf 1}-{\bf s}$ the
update law simplifies to
\begin{equation}
  \tilde{x} = ({\bf 1}-{\bf s})\bar{y}+{\bf s}\tilde{x}^{-}
\end{equation}
Here it is clear that if ${\bf s} = {\bf 1}$ the new estimate will be
equal to the old estimate meaning that the sensor noise is approaching
infinite. If ${\bf s} = 0$ it means that the new estimate is equal to
the sensor measurement meaning the model noise is approaching
infinity. This is a simple crude first order filter that can be used
when only a simple understanding of covariance is known.

\subsection{Sequential Linear Estimator}

In the above two scenarios, it is assumed that all measurements from 1
to $M$ are known at the same time instant $t$ and thus the least squares estimate can be done ``all
at once". For discrete time sensors on board a spacecraft this is
not possible. For example, if we take the weighted least squares
solution assuming we have a {\it 0th} batch of measurements, the
estimate of $\tilde{x}$ would be
\begin{equation}
  \tilde{x}_0 = ({\bf h}_0^T{\bf w}_0{\bf h}_0)^{-1}{\bf h}_0^T{\bf
    w}_0\bar{y}_0
\end{equation}
If we then waited $\Delta t$ seconds for a new set of measurements we
would have to obtain a new estimate of $\tilde{x}$ which could be done
using the equation below
\begin{equation}
  \tilde{x}_1 = ({\bf h}_1^T{\bf w}_1{\bf h}_1)^{-1}{\bf h}_1^T{\bf
    w}_1\bar{y}_1
\end{equation}
This solution however would only take into account the new
measurements. Thus, if larger matrices were constructed like
${\bf H} = [{\bf h}_0,{\bf h}_1]$ the solution for $\tilde{x}$ becomes
the same as it was in Equation \ref{e:weights}. This process would be
tedious if these matrices were computed over and over again. This is
because the matrices would continue to grow larger and larger over
time and eventually overflow the memory management system on the
computer. Thus, a method for updating the state vector every time
a new measurement is obtained must be derived. To do this the two
equations are substituted into equation \ref{e:weights}. Then a
covariance matrix is used such that ${\bf p} = ({\bf h}^T{\bf w}{\bf h})^{-1}$ 
which never grows in size. Using that simplification and making use of
a estimation gain matrix ${\bf k}$, the estimation algorithm is as follows:
\begin{enumerate}[itemsep=-5pt]
    \item The first measurement is obtained $\bar{y}_0$
    \item Compute the matrix ${\bf p_0} = ({\bf h}_0^T{\bf w_0}{\bf h}_0)^{-1}$
    \item Obtain the estimate for $\tilde{x}_0 = {\bf p_0}{\bf
      h}_0^T{\bf w_0}\bar{y}_0$ (Notice that if you use the equation
      above this is the same solution as the weighted least squares estimate)
    \item Every time a new measurement, $\bar{y}_k$, is obtained use the recursive least squares update law shown in the equation below.  
\end{enumerate}
\begin{equation}
  \begin{matrix}
    {\bf k}_{k+1} = {\bf p}_{k}{\bf h}_{k+1}^T[{\bf h}_{k+1}{\bf p}_k{\bf h}_{k+1}^T+{\bf w}_k^{-1}]^{-1}\\
    {\bf p}_{k+1} = [{\bf 1} - {\bf k}_{k+1}{\bf h}_{k+1}]{\bf p}_k\\
    \tilde{x}_{k+1} = \tilde{x}_k + {\bf k}_{k+1}(\bar{y}_{k+1} - {\bf h}_{k+1}\tilde{x}_k) \\
  \end{matrix}
\end{equation}
In the special case where the weighting matrix
${\bf w}_k$ is equal to a constant ${\bf w}$ and the state vector is
directly measured such that ${\bf h}_k$ is also identity, the 
sequential linear estimator gives the following simplified steps.
\begin{enumerate}[itemsep=-5pt]
    \item The first measurement is obtained $\bar{y}_0$
    \item Compute ${\bf p}_0 = {\bf w}^{-1}$
    \item Obtain the estimate for $\tilde{x}_0 = \bar{y}_0$ (this is a fault of ${\bf h}_k$ being identity) 
    \item Every time a new measurement, $\bar{y}_k$, is obtained use the recursive least squares update law shown in the equation below.  
\end{enumerate}
\begin{equation}
  \begin{matrix}
    {\bf k}_{k+1} = {\bf p}_k[{\bf p}_k + {\bf w}^{-1}]^{-1}\\ 
    {\bf p}_{k+1} = [{\bf 1} - {\bf k}_{k+1}]{\bf p}_k\\
    \tilde{x}_{k+1} = \tilde{x}_k + {\bf k}_{k+1}(\bar{y}_{k+1} - \tilde{x}_k) \\
  \end{matrix}
\end{equation}

\subsection{The Continuous Time Complimentary Filter}

In the above section a discrete sequential least squares update law
was formulated. In that derivation it is assumed that the state
estimate is held constant in between state measurements. It is
possible however to integrate a model of the state dynamics and use
that estimate in between state measurements. The is the start of a
Kalman Filter. To formulate the Continuous Time Complimentary Filter
the dynamics of the system are written such that
\begin{equation}
  \begin{matrix}
    \dot{\vec{x}} = {\bf f}\vec{x} + {\bf g}u +{\bf m}\vec{w} \\
    \vec{y} = {\bf h}\vec{x}
  \end{matrix}
\end{equation}
where the initial conditions are $\vec{x}_0$ and $\vec{w}$ is a
modeling noise term where ${\bf E}[\vec{w}\vec{w}^T]={\bf q}$ just as
was defined in the a priori estimation section. The model dynamics are
set up such that 
\begin{equation}
  \begin{matrix}
    \dot{\tilde{x}} = \tilde{{\bf f}}\tilde{x} + \tilde{{\bf g}}u + \vec{\gamma}\\
    \tilde{y} = {\bf h}\tilde{x} \\
    \bar{y} = {\bf h}\vec{x} + \vec{v}
  \end{matrix}
\end{equation}
where again $\bar{y}$ is the state measurement and $\vec{v}$ is noise
associated with the sensor where {\bf E}$[\vec{v}\vec{v}^T]=${\bf
  r}. The term $\vec{\gamma}$ is added as a psuedo control which can
be whatever we want. The idea is for $u$ to be the control input to
drive $\vec{x} \rightarrow \vec{x}_c$ while the psuedo control is for
the observer dynamics to drive $\tilde{y} \rightarrow \bar{y}$. The
model dynamics are going to deviate in between sensor measurements so
if the observer dynamics are designed properly the estimate can
converge to the measurement. Of course, this means your estimate is
only as good as your measurement noise but it is a start. To design
the psuedo control law, measurement feedback is used in the same form
as standard unity feedback control laws such that $\vec{\gamma} =
{bf k}\hat{y}$ where $\hat{y}$ is the difference between the estimate and
the measurement. The closed loop dynamics can then be written as
\begin{equation}
  \dot{\tilde{x}} = (\tilde{{\bf f}}-{\bf k}{\bf h})\tilde{x} + \tilde{{\bf g}}u
  + {\bf k}\bar{y}
\end{equation}
Looking at this equation it's hard to see the effect of the
observer. Thus the error dynamics must be investigated where
$\hat{x}=\tilde{x}-\vec{x}$. For the simple case it is assumed that
${\bf f}=\tilde{{\bf f}}$ and ${\bf g}=\tilde{{\bf g}}$. The closed
loop error dynamics can then be written as
\begin{equation}
  \dot{\hat{x}} = ({\bf f} - {\bf k}{\bf h})\hat{x} + {\bf k}\vec{v}
\end{equation}
in this case the solution to this equation is
\begin{equation}
  \hat{x}(t) = \hat{x}_0e^{({\bf f} - {\bf k}{\bf h})t} + \vec{\eta}
\end{equation}
where the term $\vec{\eta}$ is a function of the noise term ${\bf
  k}\vec{v}$. In this case, if {\bf k} is chosen to be large, the
error dynamics will be very fast but the noise term will be very
large. If {\bf k} is chosen to be very small the error dynamics will
be slow but the error term will not be a prevalent. The issue with
this filter of course comes with how to tune the gain matrix {\bf k}
which is what the Kalman filter seeks to address.
    
\subsection{The Continuous Discrete Kalman Filter}

In the case of the continuous discrete Kalman Filter, the model
dynamics are integrated just as in the complimentary filter. The only
difference is instead of using a continuous observer the state
estimate is updated every time a new measurement is obtained much like
the sequential least squares technique. First, let's write the model
dynamics as before without the observer and the measurement equations
are written such that the measurement is taken at timestep $t_k$ and
thereafter every $\Delta t$. 
\begin{equation}\label{e:model_dynamics}
  \begin{matrix}
    \dot{\tilde{x}} = \tilde{{\bf f}}\tilde{x} + \tilde{{\bf g}}u \\
    \tilde{y} = {\bf h}\tilde{x} \\
    \bar{y}_k = {\bf h_k}\vec{x}(t_k) + \vec{v}_k
  \end{matrix}
\end{equation}
The update equation is written using the continuous observer dynamics
used for the complimentary filter only in this case the update is
discrete.
\begin{equation}\label{e:state_update}
  \tilde{x}_k^+ = \tilde{x}_k^- + {\bf k}_k(\bar{y}_k-{\bf
    h_k}\tilde{x}_k^-)
\end{equation}
In this case $\tilde{x}_k^+$ is the estimated state after the update
while $\tilde{x}_k^-$ is the estimate before the update. The equation
for the covariance update and the Kalman Gain matrix are identical in
that the derivation is formulated just as it was before. The equations
are shown below again only $+$ and $-$ is used to denote the matrices
before and after update.
\begin{equation}\label{e:kalman_gain}
  \begin{matrix}
  {\bf k}_{k} = {\bf p}_{k}{\bf h}_{k}^T[{\bf h}_{k}{\bf p}_k^-{\bf h}_{k}^T+{\bf r}]^{-1}\\
  {\bf p}_{k}^+ = [{\bf 1} - {\bf k}_{k}{\bf h}_{k}]{\bf p}_k^-
  \end{matrix}
\end{equation}
In the sequential linear estimator however, the covariance matrix was
set using a weighted least squares approach. In this case the
covariance matrix is set such that ${\bf p} = {\bf
  E}[\hat{x}\hat{x}^T]$. Taking a derivative of this equation and
substituting in the closed loop error dynamics yields the covariance
propagation equation shown below.
\begin{equation}\label{e:covariance_dynamics}
  \dot{{\bf p}} = \tilde{{\bf f}}{\bf p} + {\bf p}\tilde{{\bf f}}^T +
      {\bf m}{\bf q}{\bf m}^T
\end{equation}
The final Continuous Discrete Kalman Filter then goes like this.
\begin{enumerate}
  \item Integrate the model dynamics in Equation
    \ref{e:model_dynamics} and the covariance dynamics in equation
    \ref{e:covariance_dynamics}
  \item When a measurement is received, the Kalman Gain matrix is
    computed using equation \ref{e:kalman_gain}. 
  \item Equation \ref{e:kalman_gain} is also used to update the
    covariance matrix
  \item Finally, equation \ref{e:state_update} is used to update the
    state vector estimate and then the process repeats.
\end{enumerate}
An example figure is shown below for a first order system. In this
figure the blue stars represent discrete sensor measurements with some
noise. Everytime the sensor is updated the model performs and update
and instantaneously changes to a new value. The model then integrates
(incorrectly due to model mismatch) until a new sensor measurement is
obtained. In this case the model is so inaccurate it makes more sense
to update the sensor more frequently or perform some sort of adaptive
control algorithm to estimate the plant dynamics.
\begin{figure}[H]
  \begin{center}
    \includegraphics[height=80mm, width=140mm]{Figures/Kalman_Filter_Example.png}
  \end{center}
  \caption{First Order Kalman Filter Example}\label{f:kalman}
\end{figure}
    
\subsection{Kalman Filter for Spacecraft Dynamics}

Attitude estimation involves a combination of attitude determination
and state estimation. Assuming at time $t=t_0$ the attitude estimation
algorithm is performed and an estimate of the quaternion is obtained
as $\tilde{q}_0$. If discrete regular angular velocity
($\bar{\omega}_{k}$) measurements are made every $\Delta t$ seconds,
the quaternion can be estimated by simply integrating the attitude
equations of motion. Even if perfect sensor measurements are made, it
is possible to integrate these equations of motion over time and the
quaternion $\vec{q}$ will be much different than the estimated
quaternion $\tilde{q}$. Thus, the attitude estimation algorithm can
run again to obtain a new absolute quaternion measurement. The
equations of motion are integrated and when a new sensor measurement
is obtained the estimated state is updated based on the estimated
covariance combined with and estimate of model errors and sensor
errors. Finally, it is possible to 
create an Extended State Kalman Filter (EKF) which can estimate
sensor inaccuracies simply by finding the least squares solution
between the sensor measurements and state estimates. The sections that
follow details the Kalman Filter for Spacecraft Dynamics as well as
the extended state version which estimate bias values in the rate gyro.
  
First, the 4-dimensionality of the quaternion renders the
above Kalman filter formulation to be impossible mostly because the
quaternion derivative is a 4 by 1 matrix while the angular velocity
vector is a 3 by 1. Furthermore, the quaternion derivative is not
linear and cannot be expressed as the linear matrices in the previous
section. As such the Kalman Filter must be updated somewhat. The
derivative of the state $\dot{\vec{q}}$ is cumbersome and follows the
reference in \cite{Liu_Estimation}. First the angular 
velocity measurement is substituted into the derivative of
quaternions where the ${\bf \Omega}()$ and ${\bf \chi}()$ identity is used to
separate out the white noise parameter.
\begin{equation}
  \dot{\vec{q}} = \frac{1}{2}{\bf \Omega}(\vec{\omega})\vec{q} =
  \frac{1}{2}{\bf \Omega}(\bar{\omega}-\vec{b}-\vec{\eta}_g)\vec{q} =
  \frac{1}{2}{\bf \Omega}(\bar{\omega}-\vec{b})\vec{q} - \frac{1}{2}{\bf \chi}(\vec{q})\vec{\eta}_g
\end{equation}
At this point an error quaternion is created using the difference
between $\vec{q}$ and $\tilde{q}$. Recall that the error quaternion is
given by the equation below. The full equation is shown in
\ref{e:quat_difference}. 
\begin{equation}
  \delta \vec{q} = \vec{q}~\Earth~\tilde{q}^{-1}
\end{equation}
The derivative of this difference quaternion is beyond the scope of
this report but can be found in \cite{kalman_quat}.
\begin{equation}
  \dot{\delta \vec{q}} = \begin{Bmatrix} 0 \\ -{\bf
      S}(\tilde{\omega})\delta \vec{\epsilon} \end{Bmatrix} +
  \frac{1}{2}{\bf \Omega}(\delta \vec{\omega})\delta \vec{q}
\end{equation}
where $\delta \vec{\omega} = \vec{\omega} - \tilde{\omega}$ and
$\delta \vec{\epsilon} = \vec{\epsilon} - \tilde{\epsilon}$. Recall
that $\tilde{\omega} = \bar{\omega}-\tilde{b}$. The second term in the
equation above can be expanded using the equations in Section
\ref{s:measurements}. Note that $\delta \vec{\omega}$ simplifies to
$-\delta \vec{b} - \vec{\eta}_g$ and $\dot{\delta \vec{q}} = [\dot{\delta q_0},\dot{\delta
    \vec{\epsilon}}]^T$. 
\begin{equation}
  \dot{\delta \vec{q}} = \begin{Bmatrix} 0 \\ -{\bf
      S}(\tilde{\omega})\delta \vec{\epsilon} \end{Bmatrix} -
  \frac{1}{2}\begin{Bmatrix} -\delta \vec{\epsilon}^T\delta \vec{b}
    \\ \delta q_0 \delta \vec{b} + {\bf S}(\delta
    \vec{\epsilon})\delta \vec{b} \end{Bmatrix} -
  \frac{1}{2}\begin{Bmatrix} -\delta \vec{\epsilon} \vec{\eta}_g
    \\ \delta q_0 \vec{\eta}_g + {\bf S}(\delta
    \vec{\epsilon})\vec{\eta}_g \end{Bmatrix}
\end{equation}
In order to proceed further, small angle
approximations are made such that $|\delta \vec{q}|<<1$. The latter 3
variables in the quaternion are further approximated as $\delta
\vec{\rho} = \delta \vec{\epsilon}$. In order to fit in with the standard
Kalman filter, the state vector $\vec{x} = \vec{\rho}$ and thus the state
dynamics $\dot{\delta \vec{x}}$ can then be written as
\begin{equation}
  \dot{\delta \vec{x}} = \dot{\delta \vec{\rho}} = -{\bf
    S}(\tilde{\omega})\delta \vec{d} - \frac{1}{2}\delta \vec{b} -
  \frac{1}{2}\vec{\eta}_g
\end{equation}
In order to extract the attitude quaternion from the approximated
state the following equations are used.
\begin{equation}
  \begin{matrix}
    \delta \vec{\epsilon} = \frac{\delta \vec{\rho}}{\sqrt{1+\delta
        \vec{\rho}^T\delta \vec{\rho}}} & q_0 = \frac{1}{\sqrt{1+\delta
        \vec{\rho}^T\delta \vec{\rho}}}
  \end{matrix}
\end{equation}

\subsection{Extended State Kalman Filter}

As shown in the previous section, a Kalman filter can be used to
estimate the state. The standard Kalman filter however can be extended
to include the bias of the angular velocity measurement. Thus the
state vector is augmented to be $\vec{x} = [\vec{q},\vec{b}]^T$. Since
the derivative of the bias is the white noise vector, the difference
state vector after much simplification is shown below.
\begin{equation}
  \dot{\delta \vec{x}} = \begin{Bmatrix} \dot{\delta \vec{\rho}}
    \\ \delta \vec{b} \end{Bmatrix} =
  \begin{bmatrix} -{\bf S}(\tilde{\omega}) & -\frac{1}{2}{\bf I}_{3x3} \\
    {\bf 0}_{3x3} & {\bf 0}_{3x3} \end{bmatrix}
  \begin{Bmatrix} \delta \vec{\rho} \\\delta \vec{b} \end{Bmatrix} +
  \begin{Bmatrix} -\frac{1}{2}\vec{\eta}_g \\ \vec{\eta}_b \end{Bmatrix}
\end{equation}
In this formulation $\delta \vec{b} = \vec{b} - \tilde{b}$. The
derivative is then $\delta \dot{\vec{b}} = \vec{\eta}_b - 0$. It is
assumed that the derivative of the estimate is zero and thus is only
updated when sensor measurements are made. The states equation above can be
reduced to the state space form shown below. 
\begin{equation}
  \dot{\delta \vec{x}} = {\bf A}\delta \vec{x} + \vec{\eta}
\end{equation}
